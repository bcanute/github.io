
  Machine Learning Project


        /Brian Canute/


        /Sunday, September 06, 2015/

Machine Learning is a realtively young, technology that has emerged from
the traditional area of statistics aided greatly by the recent computer
revolution. Canonical concepts and vocabulary are still being defined
but it is generally agreed that a machine learning model succeeds
through its ability to predict rather than explain.

This paper will therefore treat the machine metrics as a black box; 159
columns of mainly dynamic spatial data that predicts column 160.

The outcome variable in column 160 relates to six young health
participants who were asked to perform one set of 10 repetitions of the
Unilateral Dumbbell Biceps Curl at five different qualitative levels of
human movement expertise:
(Class A): exactly according to the specification,
(Class B): throwing the elbows to the front,
(Class C): lifting the dumbbell only halfway,
(Class D): lowering the dumbbell only halfway, and
(Class E): throwing the hips to the front.

More details can be found at:
http://groupware.les.inf.puc-rio.br/har#ixzz3ktRql3d3


    Raw Data and R Library Packages Used

opts_chunk$set(warning=FALSE)

|## Warning: package 'caret' was built under R version 3.1.3|

|## Loading required package: lattice
## Loading required package: ggplot2|

|## Warning: package 'rpart.plot' was built under R version 3.1.3|

|## Warning: package 'rattle' was built under R version 3.1.3|

|## Loading required package: RGtk2
## Rattle: A free graphical interface for data mining with R.
## Version 3.5.0 Copyright (c) 2006-2015 Togaware Pty Ltd.
## Type 'rattle()' to shake, rattle, and roll your data.|

|## Warning: package 'e1071' was built under R version 3.1.3|

|## Warning: package 'randomForest' was built under R version 3.1.3|

|## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.|


    Tidying the Data

|## [1] TRUE|

|## [1]  0 20|

|## [1] 20 53|

|## [1] 19622    53|


    Split the Training Set


    Cart Tree Model

|## CART 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 
## Resampling results across tuning parameters:
## 
##   cp      Accuracy  Kappa  Accuracy SD  Kappa SD
##   0.0366  0.515     0.367  0.0222       0.0354  
##   0.0595  0.434     0.237  0.0655       0.1082  
##   0.1179  0.346     0.091  0.0369       0.0580  
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.0366.|

|##           Reference
## Prediction    A    B    C    D    E
##          A 1510   27  132    0    5
##          B  463  389  287    0    0
##          C  462   36  528    0    0
##          D  427  163  374    0    0
##          E  179  141  299    0  463|

|## Accuracy 
## 0.491079|


    Random Forest Model

|## Random Forest 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 10989, 10990, 10989, 10989, 10991 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
##    2    0.990     0.987  0.00273      0.00345 
##   27    0.991     0.989  0.00142      0.00180 
##   52    0.984     0.980  0.00137      0.00173 
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 27.|

|##           Reference
## Prediction    A    B    C    D    E
##          A 1671    0    2    0    1
##          B    7 1128    3    1    0
##          C    0    4 1019    3    0
##          D    0    0    5  957    2
##          E    0    1    2    1 1078|

|##  Accuracy 
## 0.9945624|


    Boosting Model

|## Loading required package: gbm|

|## Warning: package 'gbm' was built under R version 3.1.3|

|## Loading required package: survival
## Loading required package: splines
## 
## Attaching package: 'survival'
## 
## The following object is masked from 'package:caret':
## 
##     cluster
## 
## Loading required package: parallel
## Loaded gbm 2.1.1|

|## Stochastic Gradient Boosting 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD
##   1                   50      0.747     0.679  0.00823      0.01033 
##   1                  100      0.817     0.768  0.00538      0.00669 
##   1                  150      0.849     0.809  0.00536      0.00671 
##   2                   50      0.851     0.812  0.00568      0.00714 
##   2                  100      0.902     0.875  0.00472      0.00591 
##   2                  150      0.926     0.906  0.00491      0.00617 
##   3                   50      0.892     0.863  0.00605      0.00760 
##   3                  100      0.936     0.919  0.00474      0.00596 
##   3                  150      0.955     0.943  0.00338      0.00425 
## 
## Tuning parameter 'shrinkage' was held constant at a value of 0.1
## 
## Tuning parameter 'n.minobsinnode' was held constant at a value of 10
## Accuracy was used to select the optimal model using  the largest value.
## The final values used for the model were n.trees = 150,
##  interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.|

|##           Reference
## Prediction    A    B    C    D    E
##          A 1637   22    8    4    3
##          B   39 1074   24    1    1
##          C    0   30  986    8    2
##          D    2    2   24  927    9
##          E    1   11   12    9 1049|

|##  Accuracy 
## 0.9639762|


    Submit Predictions on the Test Set

|##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E|

